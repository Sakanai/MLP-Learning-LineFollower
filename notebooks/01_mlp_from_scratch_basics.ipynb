{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qa8voAT80dE",
        "outputId": "ffd92d45-aac7-4da8-b211-d1a09b2a6940"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# INPUT DARI USER -- JARINGAN 1 LAYER\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"NEURAL NETWORK CALCULATOR - 1 LAYER (wX + b)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Input dari user\n",
        "X = float(input(\"Masukkan nilai input X (contoh: 2.0): \"))\n",
        "y = float(input(\"Masukkan nilai target y (contoh: 3.0): \"))\n",
        "w = float(input(\"Masukkan nilai weight awal w (contoh: 0.5): \"))\n",
        "b = float(input(\"Masukkan nilai bias awal b (contoh: 1.0): \"))\n",
        "learning_rate = float(input(\"Masukkan learning rate α (contoh: 0.1): \"))\n",
        "iterations = int(input(\"Berapa iterasi? (contoh: 5): \"))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"PARAMETER AWAL:\")\n",
        "print(f\"X = {X}, y = {y}\")\n",
        "print(f\"w = {w}, b = {b}, α = {learning_rate}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================\n",
        "# FUNGSI PERHITUNGAN\n",
        "# ============================================\n",
        "def neural_network_1layer(X, y, w, b, alpha, iterations):\n",
        "    \"\"\"Jaringan saraf 1 layer sederhana\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PROSES PERHITUNGAN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        print(f\"\\n--- ITERASI {i+1} ---\")\n",
        "\n",
        "        # ========= FORWARD PASS =========\n",
        "        print(f\"\\n[1] FORWARD PASS (Prediksi):\")\n",
        "        z = w * X + b\n",
        "        y_pred = z  # Linear activation\n",
        "        loss = 0.5 * (y_pred - y) ** 2\n",
        "\n",
        "        print(f\"   z = w × X + b = {w:.4f} × {X} + {b:.4f}\")\n",
        "        print(f\"     = {w*X:.4f} + {b:.4f} = {z:.4f}\")\n",
        "        print(f\"   ŷ = z = {y_pred:.4f}\")\n",
        "        print(f\"   Loss = ½(ŷ - y)² = ½({y_pred:.4f} - {y})²\")\n",
        "        print(f\"        = ½({y_pred-y:.4f})² = {loss:.6f}\")\n",
        "\n",
        "        # ========= BACKPROPAGATION =========\n",
        "        print(f\"\\n[2] BACKPROPAGATION (Gradien):\")\n",
        "\n",
        "        # Hitung gradien\n",
        "        dL_dy_pred = y_pred - y  # ∂L/∂ŷ\n",
        "        dy_pred_dz = 1.0  # ∂ŷ/∂z (linear)\n",
        "        dz_dw = X  # ∂z/∂w = X\n",
        "        dz_db = 1.0  # ∂z/∂b = 1\n",
        "\n",
        "        # Chain rule\n",
        "        dL_dw = dL_dy_pred * dy_pred_dz * dz_dw  # ∂L/∂w\n",
        "        dL_db = dL_dy_pred * dy_pred_dz * dz_db  # ∂L/∂b\n",
        "\n",
        "        print(f\"   ∂L/∂ŷ = ŷ - y = {y_pred:.4f} - {y} = {dL_dy_pred:.4f}\")\n",
        "        print(f\"   ∂ŷ/∂z = {dy_pred_dz:.4f} (karena fungsi linear)\")\n",
        "        print(f\"   ∂z/∂w = X = {dz_dw}\")\n",
        "        print(f\"   ∂z/∂b = {dz_db:.4f}\")\n",
        "        print(f\"\\n   ∂L/∂w = (∂L/∂ŷ) × (∂ŷ/∂z) × (∂z/∂w)\")\n",
        "        print(f\"         = {dL_dy_pred:.4f} × 1 × {X} = {dL_dw:.4f}\")\n",
        "        print(f\"   ∂L/∂b = (∂L/∂ŷ) × (∂ŷ/∂z) × (∂z/∂b)\")\n",
        "        print(f\"         = {dL_dy_pred:.4f} × 1 × 1 = {dL_db:.4f}\")\n",
        "\n",
        "        # ========= UPDATE PARAMETER =========\n",
        "        print(f\"\\n[3] UPDATE PARAMETER (α = {alpha}):\")\n",
        "\n",
        "        w_old = w\n",
        "        b_old = b\n",
        "\n",
        "        w = w - alpha * dL_dw\n",
        "        b = b - alpha * dL_db\n",
        "\n",
        "        print(f\"   w_new = w_old - α × ∂L/∂w\")\n",
        "        print(f\"         = {w_old:.4f} - {alpha} × ({dL_dw:.4f})\")\n",
        "        print(f\"         = {w_old:.4f} - {alpha*dL_dw:.4f} = {w:.4f}\")\n",
        "        print(f\"\\n   b_new = b_old - α × ∂L/∂b\")\n",
        "        print(f\"         = {b_old:.4f} - {alpha} × ({dL_db:.4f})\")\n",
        "        print(f\"         = {b_old:.4f} - {alpha*dL_db:.4f} = {b:.4f}\")\n",
        "\n",
        "        # ========= VERIFIKASI =========\n",
        "        print(f\"\\n[4] VERIFIKASI SETELAH UPDATE:\")\n",
        "        y_pred_new = w * X + b\n",
        "        loss_new = 0.5 * (y_pred_new - y) ** 2\n",
        "\n",
        "        print(f\"   ŷ_new = {w:.4f} × {X} + {b:.4f} = {y_pred_new:.4f}\")\n",
        "        print(f\"   Loss_new = ½({y_pred_new:.4f} - {y})² = {loss_new:.6f}\")\n",
        "\n",
        "        improvement = ((loss - loss_new) / loss) * 100 if loss > 0 else 0\n",
        "        print(f\"   Perbaikan: {loss:.6f} → {loss_new:.6f} ({improvement:.2f}%)\")\n",
        "\n",
        "    return w, b, y_pred_new, loss_new\n",
        "\n",
        "# ============================================\n",
        "# EKSEKUSI PERHITUNGAN\n",
        "# ============================================\n",
        "w_final, b_final, y_pred_final, loss_final = neural_network_1layer(\n",
        "    X, y, w, b, learning_rate, iterations\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# HASIL AKHIR\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HASIL AKHIR\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nSetelah {iterations} iterasi:\")\n",
        "print(f\"w_final = {w_final:.6f}\")\n",
        "print(f\"b_final = {b_final:.6f}\")\n",
        "print(f\"ŷ_final = w × X + b = {w_final:.6f} × {X} + {b_final:.6f} = {y_pred_final:.6f}\")\n",
        "print(f\"y_target = {y}\")\n",
        "print(f\"Error = |{y_pred_final:.6f} - {y}| = {abs(y_pred_final - y):.6f}\")\n",
        "print(f\"Loss_final = {loss_final:.6f}\")\n",
        "\n",
        "# ============================================\n",
        "# KESIMPULAN\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KESIMPULAN\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n1. Jaringan telah belajar mendekati target y={y}\")\n",
        "print(f\"2. Prediksi akhir: {y_pred_final:.4f} (target: {y})\")\n",
        "print(f\"3. Error: {abs(y_pred_final - y):.4f}\")\n",
        "print(f\"4. Parameter akhir: w={w_final:.4f}, b={b_final:.4f}\")\n",
        "\n",
        "if abs(y_pred_final - y) < 0.01:\n",
        "    print(f\"\\n✅ SUKSES! Error < 0.01\")\n",
        "elif abs(y_pred_final - y) < 0.1:\n",
        "    print(f\"\\n⚠️  CUKUP BAIK! Error < 0.1\")\n",
        "else:\n",
        "    print(f\"\\n❌ PERLU LEBIH BANYAK ITERASI ATAU α YANG LEBIH BESAR\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ============================================\n",
        "# OPTIONAL: TAMPILKAN SEMUA ITERASI DALAM TABEL\n",
        "# ============================================\n",
        "if input(\"\\nTampilkan tabel semua iterasi? (y/n): \").lower() == 'y':\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RINGKASAN SEMUA ITERASI\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Reset parameter\n",
        "    w_reset, b_reset = float(input(\"w awal: \")), float(input(\"b awal: \"))\n",
        "\n",
        "    print(f\"\\n{'Iter':^5} | {'w':^10} | {'b':^10} | {'ŷ':^10} | {'Loss':^12} | {'∂L/∂w':^10} | {'∂L/∂b':^10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    w_current, b_current = w_reset, b_reset\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Forward\n",
        "        z = w_current * X + b_current\n",
        "        y_pred = z\n",
        "        loss = 0.5 * (y_pred - y) ** 2\n",
        "\n",
        "        # Gradien\n",
        "        dL_dy_pred = y_pred - y\n",
        "        dL_dw = dL_dy_pred * X\n",
        "        dL_db = dL_dy_pred * 1\n",
        "\n",
        "        print(f\"{i+1:^5} | {w_current:^10.4f} | {b_current:^10.4f} | {y_pred:^10.4f} | {loss:^12.6f} | {dL_dw:^10.4f} | {dL_db:^10.4f}\")\n",
        "\n",
        "        # Update untuk iterasi berikutnya\n",
        "        w_current = w_current - learning_rate * dL_dw\n",
        "        b_current = b_current - learning_rate * dL_db\n",
        "\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYuJN7KiHJkX",
        "outputId": "f1bc0cd9-c3bb-458a-c597-012cbf7cdcf9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# INPUT DARI USER - JARING3AN 2 LAYER (1-1-1)\n",
        "# ============================================\n",
        "print(\"=\" * 70)\n",
        "print(\"NEURAL NETWORK 2 LAYERS: Input(1) → Hidden(1) → Output(1)\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Input dari user\n",
        "X = float(input(\"Masukkan nilai input X (contoh: 2.0): \"))\n",
        "y = float(input(\"Masukkan nilai target y (contoh: 3.0): \"))\n",
        "\n",
        "print(\"\\n--- Parameter Layer 1 (Input → Hidden) ---\")\n",
        "w1 = float(input(\"Masukkan weight w1 (contoh: 0.5): \"))\n",
        "b1 = float(input(\"Masukkan bias b1 (contoh: 1.0): \"))\n",
        "\n",
        "print(\"\\n--- Parameter Layer 2 (Hidden → Output) ---\")\n",
        "w2 = float(input(\"Masukkan weight w2 (contoh: 0.3): \"))\n",
        "b2 = float(input(\"Masukkan bias b2 (contoh: 0.5): \"))\n",
        "\n",
        "print(\"\\n--- Fungsi Aktivasi ---\")\n",
        "print(\"1. ReLU\")\n",
        "print(\"2. Sigmoid\")\n",
        "print(\"3. Linear\")\n",
        "print(\"4. Tanh\")\n",
        "print(\"5. Leaky ReLU (alpha=0.01)\")\n",
        "activation_choice = int(input(\"Pilih fungsi aktivasi (1-5): \"))\n",
        "\n",
        "learning_rate = float(input(\"\\nMasukkan learning rate α (contoh: 0.1): \"))\n",
        "iterations = int(input(\"Berapa iterasi? (contoh: 5): \"))\n",
        "\n",
        "# ============================================\n",
        "# FUNGSI AKTIVASI DAN TURUNANNYA\n",
        "# ============================================\n",
        "def relu(x):\n",
        "    return max(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return 1.0 if x > 0 else 0.0\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return 1.0\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1.0 - np.tanh(x)**2\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return x if x > 0 else alpha * x\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return 1.0 if x > 0 else alpha\n",
        "\n",
        "# Pilih fungsi berdasarkan input\n",
        "if activation_choice == 1:\n",
        "    activation = relu\n",
        "    activation_deriv = relu_derivative\n",
        "    act_name = \"ReLU\"\n",
        "elif activation_choice == 2:\n",
        "    activation = sigmoid\n",
        "    activation_deriv = sigmoid_derivative\n",
        "    act_name = \"Sigmoid\"\n",
        "elif activation_choice == 3:\n",
        "    activation = linear\n",
        "    activation_deriv = linear_derivative\n",
        "    act_name = \"Linear\"\n",
        "elif activation_choice == 4:\n",
        "    activation = tanh\n",
        "    activation_deriv = tanh_derivative\n",
        "    act_name = \"Tanh\"\n",
        "elif activation_choice == 5:\n",
        "    activation = lambda x: leaky_relu(x, 0.01)\n",
        "    activation_deriv = lambda x: leaky_relu_derivative(x, 0.01)\n",
        "    act_name = \"Leaky ReLU (α=0.01)\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARAMETER AWAL:\")\n",
        "print(f\"X = {X}, y = {y}\")\n",
        "print(f\"Layer 1: w1 = {w1}, b1 = {b1}\")\n",
        "print(f\"Layer 2: w2 = {w2}, b2 = {b2}\")\n",
        "print(f\"Aktivasi Hidden: {act_name}\")\n",
        "print(f\"α = {learning_rate}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "history = []\n",
        "\n",
        "# ============================================\n",
        "# PROSES TRAINING PER ITERASI\n",
        "# ============================================\n",
        "print(\"---------- PROSES TRAINING DETAIL ----------\")\n",
        "\n",
        "for iter_num in range(1, iterations + 1):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"ITERASI {iter_num}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # ============ FORWARD PASS ============\n",
        "    print(f\"\\n[1] FORWARD PASS:\")\n",
        "\n",
        "    # Layer 1: Input → Hidden\n",
        "    z1 = w1 * X + b1\n",
        "    a1 = activation(z1)\n",
        "\n",
        "    print(f\"    z₁ = w₁ × X + b₁ = {w1:.4f}x{X} + {b1:.4f} = {z1:.4f}\")\n",
        "    print(f\"    a₁ = {act_name}(z₁) = {act_name}({z1:.4f}) = {a1:.4f}\")\n",
        "\n",
        "    # Layer 2: Hidden → Output\n",
        "    z2 = w2 * a1 + b2\n",
        "    y_pred = z2  # Linear activation untuk output\n",
        "\n",
        "    print(f\"    z₂ = w₂ × a₁ + b₂ = {w2:.4f}x{a1:.4f} + {b2:.4f} = {z2:.4f}\")\n",
        "    print(f\"    ŷ = z₂ = {y_pred:.4f}\")\n",
        "\n",
        "    # Hitung Loss\n",
        "    loss = 0.5 * (y_pred - y) ** 2\n",
        "    print(f\"    Loss = ½(ŷ - y)² = ½({y_pred:.4f} - {y})² = {loss:.6f}\")\n",
        "\n",
        "    # ============ BACKPROPAGATION ============\n",
        "    print(f\"\\n[2] BACKPROPAGATION:\")\n",
        "\n",
        "    # Gradien untuk output layer\n",
        "    dL_dy_pred = y_pred - y\n",
        "    dy_pred_dz2 = 1.0\n",
        "    dz2_dw2 = a1\n",
        "    dz2_db2 = 1.0\n",
        "    dz2_da1 = w2\n",
        "\n",
        "    dL_dw2 = dL_dy_pred * dy_pred_dz2 * dz2_dw2\n",
        "    dL_db2 = dL_dy_pred * dy_pred_dz2 * dz2_db2\n",
        "\n",
        "    print(f\"    ∂L/∂ŷ = ŷ - y = {y_pred:.4f} - {y} = {dL_dy_pred:.4f}\")\n",
        "    print(f\"    ∂L/∂w₂ = (∂L/∂ŷ) × ∂ŷ/∂z₂ × ∂z₂/∂w₂ = {dL_dy_pred:.4f} × 1 × {a1:.4f} = {dL_dw2:.4f}\")\n",
        "    print(f\"    ∂L/∂b₂ = (∂L/∂ŷ) × ∂ŷ/∂z₂ × ∂z₂/∂b₂ = {dL_dy_pred:.4f} × 1 × 1 = {dL_db2:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Gradien untuk hidden layer\n",
        "    dL_da1 = dL_dy_pred * dy_pred_dz2 * dz2_da1\n",
        "    da1_dz1 = activation_deriv(z1)\n",
        "    dz1_dw1 = X\n",
        "    dz1_db1 = 1.0\n",
        "\n",
        "    dL_dz1 = dL_da1 * da1_dz1\n",
        "    dL_dw1 = dL_dz1 * dz1_dw1\n",
        "    dL_db1 = dL_dz1 * dz1_db1\n",
        "\n",
        "    print(f\"    ∂L/∂a₁ = (∂L/∂ŷ) × ∂ŷ/∂z₂ × ∂z₂/∂a₁ = {dL_dy_pred:.4f} × 1 × {w2:.4f} = {dL_da1:.4f}\")\n",
        "    print(f\"    ∂a₁/∂z₁ = {act_name}'({z1:.4f}) = {da1_dz1:.4f}\")\n",
        "    print(f\"    ∂L/∂z₁ = (∂L/∂a₁) × (∂a₁/∂z₁) = {dL_da1:.4f} × {da1_dz1:.4f} = {dL_dz1:.4f}\")\n",
        "    print(f\"    ∂L/∂w₁ = (∂L/∂z₁) × (∂z₁/∂w₁) = {dL_dz1:.4f} × {X} = {dL_dw1:.4f}\")\n",
        "    print(f\"    ∂L/∂b₁ = (∂L/∂z₁) × (∂z₁/∂b₁) = {dL_dz1:.4f} × 1 = {dL_db1:.4f}\")\n",
        "\n",
        "    # ============ UPDATE PARAMETER ============\n",
        "    print(f\"\\n[3] UPDATE PARAMETER (α = {learning_rate}):\")\n",
        "\n",
        "    # Simpan nilai lama\n",
        "    w1_old, b1_old, w2_old, b2_old = w1, b1, w2, b2\n",
        "\n",
        "    # Update\n",
        "    w2 = w2 - learning_rate * dL_dw2\n",
        "    b2 = b2 - learning_rate * dL_db2\n",
        "    w1 = w1 - learning_rate * dL_dw1\n",
        "    b1 = b1 - learning_rate * dL_db1\n",
        "\n",
        "    print(f\"    w₂: {w2_old:.4f} → {w2:.4f}\")\n",
        "    print(f\"    b₂: {b2_old:.4f} → {b2:.4f}\")\n",
        "    print(f\"    w₁: {w1_old:.4f} → {w1:.4f}\")\n",
        "    print(f\"    b₁: {b1_old:.4f} → {b1:.4f}\")\n",
        "\n",
        "    # ============ SIMPAN HISTORY ============\n",
        "    history.append({\n",
        "        'iter': iter_num,\n",
        "        'w1': w1_old, 'b1': b1_old,\n",
        "        'w2': w2_old, 'b2': b2_old,\n",
        "        'z1': z1, 'a1': a1,\n",
        "        'y_pred': y_pred, 'loss': loss,\n",
        "        'dL_dw1': dL_dw1, 'dL_db1': dL_db1,\n",
        "        'dL_dw2': dL_dw2, 'dL_db2': dL_db2\n",
        "    })\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# HASIL AKHIR\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"HASIL AKHIR\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Hitung prediksi akhir dengan parameter terakhir\n",
        "z1_final = w1 * X + b1\n",
        "a1_final = activation(z1_final)\n",
        "z2_final = w2 * a1_final + b2\n",
        "y_pred_final = z2_final\n",
        "loss_final = 0.5 * (y_pred_final - y) ** 2\n",
        "\n",
        "print(f\"\\nParameter akhir:\")\n",
        "print(f\"  w₁ = {w1:.6f}\")\n",
        "print(f\"  b₁ = {b1:.6f}\")\n",
        "print(f\"  w₂ = {w2:.6f}\")\n",
        "print(f\"  b₂ = {b2:.6f}\")\n",
        "\n",
        "print(f\"\\nPrediksi akhir:\")\n",
        "print(f\"  z₁ = {w1:.6f} × {X} + {b1:.6f} = {z1_final:.6f}\")\n",
        "print(f\"  a₁ = {act_name}({z1_final:.6f}) = {a1_final:.6f}\")\n",
        "print(f\"  z₂ = {w2:.6f} × {a1_final:.6f} + {b2:.6f} = {z2_final:.6f}\")\n",
        "print(f\"  ŷ = {y_pred_final:.6f}\")\n",
        "\n",
        "print(f\"\\nTarget: y = {y}\")\n",
        "print(f\"Error: |{y_pred_final:.6f} - {y}| = {abs(y_pred_final - y):.6f}\")\n",
        "print(f\"Loss akhir: {loss_final:.6f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TABEL RINGKASAN SEMUA ITERASI\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TABEL RINGKASAN SEMUA ITERASI\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Tabel 1: Parameter dan Prediksi\n",
        "print(f\"\\n{'Iter':^5} | {'w1':^8} | {'b1':^8} | {'w2':^8} | {'b2':^8} | {'z1':^8} | {'a1':^8} | {'ŷ':^8} | {'Loss':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for h in history:\n",
        "    print(f\"{h['iter']:^5} | {h['w1']:^8.4f} | {h['b1']:^8.4f} | {h['w2']:^8.4f} | {h['b2']:^8.4f} | \"\n",
        "          f\"{h['z1']:^8.4f} | {h['a1']:^8.4f} | {h['y_pred']:^8.4f} | {h['loss']:^10.6f}\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Tabel 2: Gradien\n",
        "print(f\"\\n{'Iter':^5} | {'∂L/∂w1':^10} | {'∂L/∂b1':^10} | {'∂L/∂w2':^10} | {'∂L/∂b2':^10} | \"\n",
        "      f\"{'|grad|':^10} | {'α×|grad|':^10}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for h in history:\n",
        "    grad_norm = np.sqrt(h['dL_dw1']**2 + h['dL_db1']**2 + h['dL_dw2']**2 + h['dL_db2']**2)\n",
        "    alpha_grad = learning_rate * grad_norm\n",
        "\n",
        "    print(f\"{h['iter']:^5} | {h['dL_dw1']:^10.4f} | {h['dL_db1']:^10.4f} | \"\n",
        "          f\"{h['dL_dw2']:^10.4f} | {h['dL_db2']:^10.4f} | \"\n",
        "          f\"{grad_norm:^10.4f} | {alpha_grad:^10.4f}\")\n",
        "\n",
        "print(\"=\" * 90)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PERBANDINGAN MULTI-AKTIVASI \n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PERBANDINGAN AKTIVASI BERBEDA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Daftar semua aktivasi\n",
        "activations_list = [\n",
        "    (\"ReLU\", relu, relu_derivative),\n",
        "    (\"Sigmoid\", sigmoid, sigmoid_derivative),\n",
        "    (\"Linear\", linear, linear_derivative),\n",
        "    (\"Tanh\", tanh, tanh_derivative),\n",
        "    (\"Leaky ReLU\", lambda x: leaky_relu(x, 0.01), lambda x: leaky_relu_derivative(x, 0.01))\n",
        "]\n",
        "\n",
        "# Parameter awal yang sama\n",
        "w1_init, b1_init, w2_init, b2_init = history[0]['w1'], history[0]['b1'], history[0]['w2'], history[0]['b2']\n",
        "\n",
        "print(f\"\\nDengan parameter awal: w1={w1_init:.4f}, b1={b1_init:.4f}, w2={w2_init:.4f}, b2={b2_init:.4f}\")\n",
        "print(f\"{'Aktivasi':^15} | {'z₁':^8} | {'a₁':^8} | {'ŷ':^8} | {'Loss':^10} | {'∂a₁/∂z₁':^10}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for act_name_comp, act_func, act_deriv in activations_list:\n",
        "    z1_comp = w1_init * X + b1_init\n",
        "    a1_comp = act_func(z1_comp)\n",
        "    y_pred_comp = w2_init * a1_comp + b2_init\n",
        "    loss_comp = 0.5 * (y_pred_comp - y) ** 2\n",
        "    deriv_comp = act_deriv(z1_comp)\n",
        "\n",
        "    print(f\"{act_name_comp:^15} | {z1_comp:^8.4f} | {a1_comp:^8.4f} | {y_pred_comp:^8.4f} | \"\n",
        "          f\"{loss_comp:^10.6f} | {deriv_comp:^10.4f}\")\n",
        "\n",
        "print(\"=\" * 75)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SELESAI\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
